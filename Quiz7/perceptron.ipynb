{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz7\n",
    "#### Peeraya Khantaruangsakul 63070501054\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [15 points. 1 hour.] Show that the OR, AND will work for a single layer perceptron, but the XOR will not find a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND: w1 = 0.50, w2 = 0.50, b = -0.25, Accuracy = 1.00\n",
      "0 && 0 = 0 ; True\n",
      "0 && 1 = 0 ; True\n",
      "1 && 0 = 0 ; True\n",
      "1 && 1 = 1 ; True\n",
      "OR: w1 = 0.50, w2 = 0.50, b = 0.25, Accuracy = 1.00\n",
      "0 && 0 = 0 ; True\n",
      "0 && 1 = 1 ; True\n",
      "1 && 0 = 1 ; True\n",
      "1 && 1 = 1 ; True\n",
      "XOR: w1 = -0.00, w2 = -0.00, b = 0.50, Accuracy = 0.25\n",
      "0 && 0 = 1 ; False\n",
      "0 && 1 = 0 ; False\n",
      "1 && 0 = 0 ; False\n",
      "1 && 1 = 0 ; True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_weights_bias(X, y):\n",
    "    X = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    w, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "    return w[:-1], w[-1]\n",
    "\n",
    "def predict(X, w, b):\n",
    "    return np.round(np.dot(X, w) + b)\n",
    "\n",
    "def calculate_accuracy(y, y_pred):\n",
    "    return np.mean(y == y_pred)\n",
    "\n",
    "# Define the input values\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Define the target values\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "y_or = np.array([0, 1, 1, 1])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Use linear algebra to find the weights and biases\n",
    "w_and, b_and = calculate_weights_bias(X, y_and)\n",
    "w_or, b_or = calculate_weights_bias(X, y_or)\n",
    "w_xor, b_xor = calculate_weights_bias(X, y_xor)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_and = predict(X, w_and, b_and)\n",
    "y_pred_or = predict(X, w_or, b_or)\n",
    "y_pred_xor = predict(X, w_xor, b_xor)\n",
    "\n",
    "# Print the weights and biases if accuracy is 1.0\n",
    "print(f\"AND: w1 = {w_and[0]:.2f}, w2 = {w_and[1]:.2f}, b = {b_and:.2f}, Accuracy = {calculate_accuracy(y_and, y_pred_and):.2f}\")\n",
    "for i in range(0, len(y_pred_and)):\n",
    "    print(f\"{X[i][0]} && {X[i][1]} = {int(y_pred_and[i])} ; {y_and[i]==y_pred_and[i]}\")\n",
    "\n",
    "print(f\"OR: w1 = {w_or[0]:.2f}, w2 = {w_or[1]:.2f}, b = {b_or:.2f}, Accuracy = {calculate_accuracy(y_or, y_pred_or):.2f}\")\n",
    "for i in range(0, len(y_pred_or)):\n",
    "    print(f\"{X[i][0]} && {X[i][1]} = {int(y_pred_or[i])} ; {y_or[i]==y_pred_or[i]}\")\n",
    "\n",
    "print(f\"XOR: w1 = {w_xor[0]:.2f}, w2 = {w_xor[1]:.2f}, b = {b_xor:.2f}, Accuracy = {calculate_accuracy(y_xor, y_pred_xor):.2f}\")\n",
    "for i in range(0, len(y_pred_xor)):\n",
    "    print(f\"{X[i][0]} && {X[i][1]} = {int(y_pred_xor[i])} ; {y_xor[i]==y_pred_xor[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [15 points. 1 hour.] Use multilayer perceptron on sklearn’s diabetes data meant for regression using 25% test data, find the R-square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared:  0.5064963575658734\n"
     ]
    }
   ],
   "source": [
    "# diabetes dataset sklearn\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "\n",
    "# split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(X_diabetes, y_diabetes, test_size=0.25, random_state=66)\n",
    "\n",
    "# multi layer perceptron regression\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100,100,100), max_iter=1000)\n",
    "mlp.fit(X_train_diabetes, y_train_diabetes)\n",
    "\n",
    "# predict\n",
    "predictions_diabetes = mlp.predict(X_test_diabetes)\n",
    "\n",
    "# R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R-squared: \", r2_score(y_test_diabetes, predictions_diabetes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [20 points. 2 hours.] Follow the instructions in the following site for MLPClassifier for MNIST data. https://towardsdatascience.com/classifying-handwritten-digits-using-a-multilayer-perceptron-classifier-mlp-bc8453655880 . Use 50,000 data samples for training and 20,000 for testing. Note you should scale your data to be 0-1 by dividing it by 255. Report your accuracy and print out the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "# import MNIST dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# load data\n",
    "# Load data\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
    "# Normalize intensity of images to make it in the range [0,1] since 255 is the max (white).\n",
    "X = X / 255.0\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47772268\n",
      "Iteration 2, loss = 0.16435982\n",
      "Iteration 3, loss = 0.12618247\n",
      "Iteration 4, loss = 0.10649038\n",
      "Iteration 5, loss = 0.09039465\n",
      "Iteration 6, loss = 0.07945557\n",
      "Iteration 7, loss = 0.07424755\n",
      "Iteration 8, loss = 0.06643571\n",
      "Iteration 9, loss = 0.05982603\n",
      "Iteration 10, loss = 0.05655218\n",
      "Iteration 11, loss = 0.04981067\n",
      "Iteration 12, loss = 0.05127314\n",
      "Iteration 13, loss = 0.04650352\n",
      "Iteration 14, loss = 0.04558377\n",
      "Iteration 15, loss = 0.04044781\n",
      "Iteration 16, loss = 0.03830433\n",
      "Iteration 17, loss = 0.03335348\n",
      "Iteration 18, loss = 0.03522754\n",
      "Iteration 19, loss = 0.03186215\n",
      "Iteration 20, loss = 0.02899062\n",
      "Iteration 21, loss = 0.03353281\n",
      "Iteration 22, loss = 0.02710012\n",
      "Iteration 23, loss = 0.03112358\n",
      "Iteration 24, loss = 0.02556351\n",
      "Iteration 25, loss = 0.02492123\n",
      "Iteration 26, loss = 0.02868124\n",
      "Iteration 27, loss = 0.02971985\n",
      "Iteration 28, loss = 0.02299533\n",
      "Iteration 29, loss = 0.01901845\n",
      "Iteration 30, loss = 0.02214198\n",
      "Iteration 31, loss = 0.02734307\n",
      "Iteration 32, loss = 0.02454845\n",
      "Iteration 33, loss = 0.02382058\n",
      "Iteration 34, loss = 0.02129953\n",
      "Iteration 35, loss = 0.01493116\n",
      "Iteration 36, loss = 0.02098672\n",
      "Iteration 37, loss = 0.02566953\n",
      "Iteration 38, loss = 0.02161220\n",
      "Iteration 39, loss = 0.01313089\n",
      "Iteration 40, loss = 0.01683517\n",
      "Iteration 41, loss = 0.01767170\n",
      "Iteration 42, loss = 0.01448168\n",
      "Iteration 43, loss = 0.01967482\n",
      "Iteration 44, loss = 0.01847350\n",
      "Iteration 45, loss = 0.01948972\n",
      "Iteration 46, loss = 0.01915191\n",
      "Iteration 47, loss = 0.01461438\n",
      "Iteration 48, loss = 0.01483923\n",
      "Iteration 49, loss = 0.02122285\n",
      "Iteration 50, loss = 0.01356890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set Accuracy: 0.994980\n",
      "Test set accuracy: 0.984520\n",
      "Confusion Matrix:\n",
      " [[4861    1   14    3    3    4    5    2    8    8]\n",
      " [   0 5550    1   14    1    2    5   10   13    0]\n",
      " [   5    4 4984   17    4    5    1   22   16    3]\n",
      " [   0    0   10 5017    0   11    0    7   16    4]\n",
      " [   3    5    6    0 4797    3    3    5    6   51]\n",
      " [   2    1    2   39    0 4450   17    1   15   11]\n",
      " [  13    2    5    0   44   17 4814    2    7    1]\n",
      " [   0    6   11   15    1    1    0 5148    9    9]\n",
      " [   5    4   12   21    2   15   10    5 4822    7]\n",
      " [   5    7    2   33   27   10    0   44   33 4783]]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train/test sets\n",
    "X_train, X_test = X[:50000], X[20000:]\n",
    "y_train, y_test = y[:50000], y[20000:]\n",
    "\n",
    "classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,20,10),\n",
    "    max_iter=100,\n",
    "    alpha=1e-4,\n",
    "    solver=\"sgd\",\n",
    "    verbose=10,\n",
    "    random_state=1,\n",
    "    learning_rate_init=0.1,\n",
    ")\n",
    "# fit the model on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set Accuracy: %f\" % classifier.score(X_train, y_train))\n",
    "print(\"Test set accuracy: %f\" % classifier.score(X_test, y_test))\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. [10 points. 0.5 hours.] Repeat problem #3, but also normalize the input data by dividing by variance on the scaled 0-1 data by using sklearn’s StandardScaler. Report your accuracy and see if it improves from problem #3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.45425788\n",
      "Iteration 2, loss = 0.21481453\n",
      "Iteration 3, loss = 0.17410422\n",
      "Iteration 4, loss = 0.17102410\n",
      "Iteration 5, loss = 0.13721921\n",
      "Iteration 6, loss = 0.10472141\n",
      "Iteration 7, loss = 0.08744720\n",
      "Iteration 8, loss = 0.08416938\n",
      "Iteration 9, loss = 0.08108691\n",
      "Iteration 10, loss = 0.07766257\n",
      "Iteration 11, loss = 0.06431345\n",
      "Iteration 12, loss = 0.06260147\n",
      "Iteration 13, loss = 0.05977419\n",
      "Iteration 14, loss = 0.06121207\n",
      "Iteration 15, loss = 0.05266282\n",
      "Iteration 16, loss = 0.05347182\n",
      "Iteration 17, loss = 0.05058064\n",
      "Iteration 18, loss = 0.04717300\n",
      "Iteration 19, loss = 0.04083716\n",
      "Iteration 20, loss = 0.03751302\n",
      "Iteration 21, loss = 0.04493942\n",
      "Iteration 22, loss = 0.06049365\n",
      "Iteration 23, loss = 0.04896608\n",
      "Iteration 24, loss = 0.04423101\n",
      "Iteration 25, loss = 0.03964135\n",
      "Iteration 26, loss = 0.03783379\n",
      "Iteration 27, loss = 0.04419121\n",
      "Iteration 28, loss = 0.03728567\n",
      "Iteration 29, loss = 0.04044066\n",
      "Iteration 30, loss = 0.03627037\n",
      "Iteration 31, loss = 0.03244111\n",
      "Iteration 32, loss = 0.03556448\n",
      "Iteration 33, loss = 0.03275667\n",
      "Iteration 34, loss = 0.03730756\n",
      "Iteration 35, loss = 0.04143472\n",
      "Iteration 36, loss = 0.03927327\n",
      "Iteration 37, loss = 0.04222316\n",
      "Iteration 38, loss = 0.06276249\n",
      "Iteration 39, loss = 0.13253835\n",
      "Iteration 40, loss = 0.11231755\n",
      "Iteration 41, loss = 0.10074762\n",
      "Iteration 42, loss = 0.15611529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set Accuracy: 0.978080\n",
      "Test set accuracy: 0.968460\n"
     ]
    }
   ],
   "source": [
    "# use standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# tranform data by variance on the scaled 0-1\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# fit the model on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set Accuracy: %f\" % classifier.score(X_train, y_train))\n",
    "print(\"Test set accuracy: %f\" % classifier.score(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d417a21d97a2e6b8832d21dc44f5cbe4be6e18325f874a8bb8484565b928cd54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
